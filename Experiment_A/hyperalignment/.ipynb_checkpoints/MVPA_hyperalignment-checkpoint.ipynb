{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpEnYL7UbCXn"
   },
   "source": [
    "## MVPA Hyperalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Donggeun Kim\n",
    "@affiliation: NYSPI, Columbia University\n",
    "@date: Oct 2018 - Oct 2020\n",
    "@overview: Generates Cross-Subject-Validation Fold datasets using ANOVA and MVPA hyperalignment.\n",
    "@input: Brain Tissue Mask (mask.nii), subject-specific minimally processed fMRI scans.\n",
    "@output: LOOCV Train/Test folds of 300-length hyperaligned voxels as numpy arrays. LOOCV Train/Test labels.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mvpa2\n",
    "import glob\n",
    "import os\n",
    "from mvpa2.suite import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z3l9jwibCYP"
   },
   "outputs": [],
   "source": [
    "datapath=r'D:\\dataprocess\\shared\\MDD\\AFNI\\MRIunit_More_Than_One_Run'\n",
    "maskname = [r'D:\\dataprocess\\shared\\MDD\\AFNI\\MRIunit_More_Than_One_Run\\E13260\\prep_firstlevel\\firstLevel_fmri_trigger_scan_9__007\\mask.img']\n",
    "\n",
    "attributename=[]\n",
    "filename=[]\n",
    "#maskname=[]\n",
    "subjectID=[]\n",
    "for root, dirs, files in os.walk(datapath):\n",
    "    if root.find('E13905')<0:\n",
    "        w_files = glob.glob(os.path.join(root,'wrast*_attributes.txt'))\n",
    "        f_files = glob.glob(os.path.join(root,'f*_attributes.txt'))\n",
    "        subjectID.append(dirs)\n",
    "        for f in w_files:\n",
    "            filename.append(os.path.abspath(f)[:-15]+'.nii')\n",
    "        for f in f_files:\n",
    "            attributename.append(os.path.abspath(f))\n",
    "    #        maskname.append(os.path.abspath(f)[:-14]+'mask.nii')\n",
    "subjectID = subjectID[0]\n",
    "subjectID.remove('E13905')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJziRxCCbCYv",
    "outputId": "692e193f-239a-4716-9ccf-8ac2e8082c49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E13260',\n",
       " 'E13341',\n",
       " 'E13495',\n",
       " 'E13544',\n",
       " 'E13550',\n",
       " 'E13571',\n",
       " 'E14081',\n",
       " 'E14133',\n",
       " 'E14146',\n",
       " 'E14163',\n",
       " 'E14178']"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3v4UOE9VbCZB"
   },
   "outputs": [],
   "source": [
    "fds = []\n",
    "for subject in subjectID:\n",
    "    attribute_subject = [attribute_subject for attribute_subject in attributename if attribute_subject.find(subject)>0]\n",
    "    file_subject = [file_subject for file_subject in filename if file_subject.find(subject)>0]\n",
    "#    mask_subject = [mask_subject for mask_subject in maskname if mask_subject.find(subject)>0]\n",
    "    fds_subject = []\n",
    "    for i in range(len(attribute_subject)):\n",
    "        bold_fname = file_subject[i]\n",
    "#        mask_fname = mask_subject[-1]\n",
    "        mask_fname=maskname[-1]\n",
    "        attr_fname = attribute_subject[i]\n",
    "        attr = SampleAttributes(attr_fname)\n",
    "        fds_subject += [fmri_dataset(samples=bold_fname,\n",
    "                    targets=attr.targets, chunks=attr.chunks,\n",
    "                    mask=mask_fname)]\n",
    "    fds += [mvpa2.base.dataset.vstack(fds_subject,a=0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFQIuUTBbCZV"
   },
   "source": [
    "#### Within Subject\n",
    "http://www.pymvpa.org/examples/hyperalignment.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcZjvl9TbCZZ",
    "outputId": "65cc6371-7016-4e50-99c8-7bf60587be01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 subjects\n",
      "Per-subject dataset: 400 samples with 47828 features\n",
      "Stimulus categories: 0, 1, 2\n",
      "Performing classification analyses...\n",
      "within-subject...\n",
      " done in 9.2 seconds\n"
     ]
    }
   ],
   "source": [
    "ds_all = fds\n",
    "_ = [zscore(ds) for ds in ds_all]\n",
    "for i, sd in enumerate(ds_all):\n",
    "    sd.sa['subject'] = np.repeat(i, len(sd))\n",
    "nsubjs = len(ds_all) # number of subjects\n",
    "ncats = len(ds_all[0].UT) # number of labels\n",
    "nruns = len(ds_all[0].UC) # number of runs\n",
    "print \"%d subjects\" % len(ds_all)\n",
    "print \"Per-subject dataset: %i samples with %i features\" % ds_all[0].shape\n",
    "print \"Stimulus categories: %s\" % ', '.join(ds_all[0].UT)\n",
    "# use same classifier\n",
    "clf = LinearCSVMC()\n",
    "\n",
    "# feature selection helpers\n",
    "nf = 300\n",
    "fselector = FixedNElementTailSelector(nf, tail='upper',\n",
    "                                      mode='select', sort=False)\n",
    "sbfs = SensitivityBasedFeatureSelection(OneWayAnova(), fselector,\n",
    "                                        enable_ca=['sensitivities'])\n",
    "# create classifier with automatic feature selection\n",
    "fsclf = FeatureSelectionClassifier(clf, sbfs)\n",
    "print \"Performing classification analyses...\"\n",
    "print \"within-subject...\"\n",
    "wsc_start_time = time.time()\n",
    "cv = CrossValidation(fsclf,\n",
    "                     NFoldPartitioner(attr='chunks'),\n",
    "                     errorfx=mean_match_accuracy)\n",
    "# store results in a sequence\n",
    "wsc_results = [cv(sd) for sd in ds_all]\n",
    "wsc_results = vstack(wsc_results)\n",
    "print \" done in %.1f seconds\" % (time.time() - wsc_start_time,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kFibucRbCZm",
    "outputId": "1152b64f-218d-4f00-8720-cd6ee5af4d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between-subject (hyperaligned)...\n",
      "done in 43.3 seconds\n"
     ]
    }
   ],
   "source": [
    "print \"between-subject (hyperaligned)...\"\n",
    "hyper_start_time = time.time()\n",
    "bsc_hyper_results = []\n",
    "# same cross-validation over subjects as before\n",
    "cv = CrossValidation(clf, NFoldPartitioner(attr='subject'),\n",
    "                     errorfx=mean_match_accuracy)\n",
    "\n",
    "# leave-one-run-out for hyperalignment training\n",
    "for test_run in range(nruns):\n",
    "    # split in training and testing set\n",
    "    ds_train = [sd[sd.sa.chunks != test_run, :] for sd in ds_all]\n",
    "    ds_test = [sd[sd.sa.chunks == test_run, :] for sd in ds_all]\n",
    "\n",
    "    # manual feature selection for every individual dataset in the list\n",
    "    anova = OneWayAnova()\n",
    "    fscores = [anova(sd) for sd in ds_train]\n",
    "    featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]\n",
    "    ds_train_fs = [fs.forward(sd) for fs, sd in zip(featsels, ds_train)]\n",
    "\n",
    "\n",
    "    # Perform hyperalignment on the training data with default parameters.\n",
    "    # Computing hyperalignment parameters is as simple as calling the\n",
    "    # hyperalignment object with a list of datasets. All datasets must have the\n",
    "    # same number of samples and time-locked responses are assumed.\n",
    "    # Hyperalignment returns a list of mappers corresponding to subjects in the\n",
    "    # same order as the list of datasets we passed in.\n",
    "\n",
    "\n",
    "    hyper = Hyperalignment()\n",
    "    hypmaps = hyper(ds_train_fs)\n",
    "\n",
    "    # Applying hyperalignment parameters is similar to applying any mapper in\n",
    "    # PyMVPA. We start by selecting the voxels that we used to derive the\n",
    "    # hyperalignment parameters. And then apply the hyperalignment parameters\n",
    "    # by running the test dataset through the forward() function of the mapper.\n",
    "\n",
    "    ds_test_fs = [fs.forward(sd) for fs, sd in zip(featsels, ds_test)]\n",
    "    ds_hyper = [h.forward(sd) for h, sd in zip(hypmaps, ds_test_fs)]\n",
    "\n",
    "    # Now, we have a list of datasets with feature correspondence in a common\n",
    "    # space derived from the training data. Just as in the between-subject\n",
    "    # analyses of anatomically aligned data we can stack them all up and run the\n",
    "    # crossvalidation analysis.\n",
    "\n",
    "    ds_hyper = vstack(ds_hyper)\n",
    "    # zscore each subject individually after transformation for optimal\n",
    "    # performance\n",
    "    zscore(ds_hyper, chunks_attr='subject')\n",
    "    res_cv = cv(ds_hyper)\n",
    "    bsc_hyper_results.append(res_cv)\n",
    "\n",
    "bsc_hyper_results = hstack(bsc_hyper_results)\n",
    "print \"done in %.1f seconds\" % (time.time() - hyper_start_time,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRIgCqEFbCZy"
   },
   "outputs": [],
   "source": [
    "#Experiments found nf=300 to be an appropriate value for the number of selected voxels.\n",
    "\n",
    "nf=300\n",
    "hyper_start_time = time.time()\n",
    "bsc_hyper_results = []\n",
    "# same cross-validation over subjects as before\n",
    "cv = CrossValidation(clf, NFoldPartitioner(attr='subject'),\n",
    "                     errorfx=mean_match_accuracy)\n",
    "anova = OneWayAnova()\n",
    "fscores = [anova(sd) for sd in ds_all]\n",
    "featsels = [StaticFeatureSelection(fselector(fscore)) for fscore in fscores]\n",
    "ds_fs = [fs.forward(sd) for fs, sd in zip(featsels, ds_all)]\n",
    "\n",
    "hyper = Hyperalignment()\n",
    "hypmaps = hyper(ds_fs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2ZY1YLlbCZ5"
   },
   "outputs": [],
   "source": [
    "ds_hyper = [h.forward(sd) for h, sd in zip(hypmaps, ds_fs)]\n",
    "ds_hyper = vstack(ds_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YJkEPIRbCaB",
    "outputId": "10c28ca3-01c8-4eb6-cf54-ddeb7dbcba62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_hyper.targets.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmTTYW_RbCaL"
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"X_hyp_v2.csv\", ds_hyper.samples, delimiter=\",\")\n",
    "np.savetxt(\"Y_hyp_v2.csv\", ds_hyper.targets.astype(int), delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Jfn_qNDbCaV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MVPA_Learning_v10_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
